---
title: "HMM"
output:
  word_document: default
  pdf_document: default
  html_document: default
---

# Some Notes on HMM's

- Introduction to Markov Models
- Hidden Markov Models
- Forward Algorithm
- Viterbi Algorithm
- Baum - Welch Algorithm
- Short Example
    - Stock Market


## Intro

A hidden markov model (HMM) is a statistical markov model in which the system being modeled is assumed to be a Markov process with unobserved (hidden) states.

In simple markov models the state is directly visible to the observer, and therefore the sate transisiton probabilities are the only parameters. In a HMM the state is not directly visible, but the output, dependent on the state is visible. Each state has a probability distribution over the possible output tokens. Therefore the sequence of tokens generated by an HMM gives some information about the sequence of states. The adjective hidden refers to the state sequence through which the model passes, not the parameters of the model

## Typical Example

In discrete form an HMM process can be visualized as a generalization of the Urn problem with replacement ( each item taken from the urn is replaced after being observed).

COnsider the example of in a room that is not visible to an observer. The room contains a number of urns $X_1,X_2,X_2,...$ where each contain a known mix of balls labeled $y_1,y_2,y_3,..$ a genie chooses an urn in that room and randomly draws a ball from that urn and puts it on a conveyer belt for you to observe. The genie has a some procedure to chose urns; the choice of the urn for the nth ball depends only upon a random number and the choice of the urn for the (n-1)th ball.

## Models

Once a system can be described as a HMM, three problems can be solved.
-Evaluation
    -Pattern recognition, finding probability of an observed sequence given a HMM.
-Decoding
    -Pattern recognition, finding a sequence of states that most probably generated an observed sequence.
-Learning
    -Generating a HMM given a sequence of observations(learning).


## Algorithms

Each of these will be solved by different algorithms 

- Evaluation is solved by Forward algorithm
- Decoding is solved by Viterbi algorithm
- Learning is solved by Baum-Welch EM algorithm

Notes are not presented in full for now on details of the algorithms.

## Stock Prediction

Find any stock you like using quandl, decode it using Viterbi algorithm and then utilize Baum-Welch Em algorithm to make some predictions.

We use UnitedHealth Group Incorporated since it was second hit on quandl.com


```{r,warning=FALSE}
library(Quandl)
UNH<- Quandl("EOD/UNH", type="zoo",api_key=("5GKyMHbYLDKNxe4h4eYe"))
UNH<-as.data.frame(UNH)
UNH$Day<-seq.int(nrow(UNH))

library(ggplot2)
ggplot(aes(x=Day,y=Close),data=UNH)+geom_line(col="red")+ggtitle("UNH 1990-03-26 to 2017-05-26")

```


Also store differnece between open and close of the day.

```{r,warning=FALSE}
UNH$Dif<-(UNH$Open - UNH$Close)
ggplot(aes(x=Day,y=Dif),data=UNH)+geom_line(col="red")+ggtitle("UNH DIfference high and low 1990-03-26 to 2017-05-26")

```

this is a stationary time series, non constant variance though these things don't matter for HMM though.

We are going to model difference between open and close.

We will use RHmm 2.0.3 package.

In this example identify 5 hidden states to the models with 4 different distributions for each hidden state.

Also specify a number of iterations for the algorithm to process to equal to 2000 to make sure it converges.

```{r,warning=FALSE}
#install.packages("C:/Users/BWAGE/Desktop/Physics_Research/RHmm", repos = NULL, type="source")
library(RHmm)

model<-HMMFit(obs=UNH$Dif,dis = "MIXTURE",nStates=5,nMixt=4,control=list(iter=2000))

print(model)
```


And there we have it folks decode the HMM from the data.

Now utilize the Viterbi algorithm to determine the hidden states of the data.

```{r,warning=FALSE}
library(quantmod)
####### Viterbi Algorithm #####
VitPath<- viterbi(model,UNH$Dif)
UNH$State<-VitPath$states

UNH<-as.xts(UNH)
chartSeries(UNH[,1])
addTA(UNH[UNH$State==1,1],on=1,col=5,pch=25)
addTA(UNH[UNH$State==2,1],on=1,col=6,pch=24)
addTA(UNH[UNH$State==3,1],on=1,col=7,pch=23)
addTA(UNH[UNH$State==4,1],on=1,col=8,pch=22)
addTA(UNH[UNH$State==5,1],on=1,col=10,pch=21)

```


What we are seeing here is the different states the stock could be in each with its own probability distribution.


Now lets make some predictions on tommorrows stock price using Baum-Welch EM algorithm


```{r,warning=FALSE}
change<-sum(model$HMM$transMat[last(VitPath$states),]*.colSums((matrix(unlist(model$HMM$distribution$mean),nrow=4,ncol=5))*
  (matrix(unlist(model$HMM$distribution$proportion),nrow=4,ncol=5)),m=4,n=5))

change

mydata <- last(UNH)

predict<- mydata$Open + change


mydata$Open


predict$Open

```

So the model predicts the stock price is going to go down a bit today.

